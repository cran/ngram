\section{Background and Utilities}\label{sec:utils}

The n-gram processor in the \pkg{ngram} package changes its behavior depending
on the way the input is formatted. If the input is given as a single string,
n-grams crossing ``sentence'' boundaries will be considered valid. To prevent
this from occuring, a vector of sentences may be used as input rather than a
single string containing the entire text.

The n-gram tokenizer in the \pkg{ngram} package accepts a custom string
containing characters to be used as word separators. There may be texts that use
a wide variety of word separators, making it impractical to generate a string
containing all of them.

The \pkg{ngram} package offers several useful utilities to simplify the text and
assist with transforming the input to get the appropriate sentence handling
behavior.

\subsection{I/O}

Generally speaking, the facilities in base \proglang{R} should be sufficient.
Specifically, \code{readLines()} is a good choice for handling I/O of plain text
files.  However, for reading in multiple files (say, all \code{.txt} files in
some directory), we offer a simple utility \code{multiread()}.  This function
will read all specified files into a named list, where the names are the
filenames, and the values are the text contained in the given file as a single
string.  

So for example, if you want to read in all files ending in \code{.txt} at some
directory/path of interest \code{path}, you could call:

\begin{lstlisting}[language=rr]
library(ngram)
multiread(path, extension="txt")
\end{lstlisting}

In \code{multiread()}, the extensions \code{*.txt}, \code{*txt}, \code{.txt},
and \code{txt} are all treated the same.



\subsection{Concatenating Multiple Strings}

Since the n-gram tokenizer expects only single strings, we offer the simple
\code{concatenate()} function:

\begin{lstlisting}[language=rr]
> str(letters)
## chr [1:26] "a" "b" "c" "d" "e" "f" "g" "h" "i" "j" ...

concatenate(letters)
## [1] "a b c d e f g h i j k l m n o p q r s t u v w x y z"

concatenate(concatenate(letters, collapse=""), letters)
## [1] "abcdefghijklmnopqrstuvwxyz a b c d e f g h i j k l m n o p q r s t u v 
w x y z"
\end{lstlisting}

So if data is coming from multiple files, the simplest way to merge them 
together would be to call \code{multiread()} (described above) if possible.
Then you can call \code{concatenate()} directly on the returned list to produce
a single string from all files, or use the tokenizer iteratively, using, say, an
\code{lapply()}.  Here is a more explicit example without the use of
\code{multiread()}:

\begin{lstlisting}[language=rr]
x <- readLines("file1")
y <- readLines("file2")

str <- concatenate(x, y)
\end{lstlisting}



\subsection{Splitting Strings}

The \pkg{ngram} tokenizer always splits words at one or more of the characters
provided in the \code{sep} argument. You can preprocess the input string with
\R's regular expression utilities, such as \code{gsub()}.  But for most tasks,
the \code{preprocess()} and \code{charsplitter} utilities in the \thispackage
package should be more than sufficient.


The \code{preprocess()} function is a simple utility for making letter case
uniform, as well as optionally splitting at punctuation (so that punctuation
itself becomes a ``word'' in the n-gram analysis).  Here is a simple example:

\begin{lstlisting}[language=rr]
x <- "Watch  out    for snakes!  "
 
preprocess(x)
## [1] "watch out for snakes!"
 
preprocess(x, case="upper", remove.punct=TRUE)
## [1] "WATCH OUT FOR SNAKES"
\end{lstlisting}

Perhaps more useful is the \code{charsplitter()} function.  Suppose that for the
purposes of n-gram tokenization, instead of wanting to call things separated by
spaces ``words'', you wish to treat every letter as a ``word''. This could be
accomplished by using \code{sep=""} when calling \code{ngram()}, but for the
sake of introducing \code{charsplitter()}, it can also be done simply in this
way:

\begin{lstlisting}[language=rr]
x <- "abacabb"
splitter(x, split.char=TRUE)
## [1] "a b a c a b b"
\end{lstlisting}

By default, this will preserve spaces as a special token (an underscore by
default).  You may wish to ignore spaces entirely during tokenization.  This too
is simple to handle during preprocessing:

\begin{lstlisting}[language=rr]
y <- "abacabb abacabb"
 
splitter(y, split.space=TRUE)
## [1] "abacabb _ abacabb"
splitter(y, split.space=FALSE, split.char=TRUE)
## [1] "a b a c a b b a b a c a b b"
splitter(y, split.space=TRUE, split.char=TRUE)
## [1] "a b a c a b b _ a b a c a b b"
\end{lstlisting}



\subsection{Dealing with tm}

The \pkg{tm} package~\citep{tm} requires that all data be in the form of its
fairly complicated \code{Corpus} object.  The \pkg{ngram} package offers no
direct methods for dealing with data in this format.  To use
\pkg{tm}-encapsulated data, you will first need to extract it into a single
string or a vector of strings depending on what processing behavior is required.

If you want to extract the text from all documents in a corpus as a single
string, you can do something like:

\begin{lstlisting}[language=rr]
str <- concatenate(lapply(myCorpus, "[", 1))   
\end{lstlisting}

%TODO vector example?



\subsection{Summarizing}

%TODO: is it worth updating this function to allow custom separators and string vectors?

While not strictly related to n-gram modeling, you may wish to get some basic
summary counts of your text.  With the assumption that the text is a single
string with words separated by one or more spaces, we can very quickly generate
these counts via the \code{string.summary()} function:

\begin{lstlisting}[language=rr]
x <- "a b a c a b b"
string.summary(x)
## Chars:       13
## Letters:     7
## Whitespace:  6
## Punctuation: 0
## Digits:      0
## Words:       7
## Sentences:   0
## Lines:       1 
## Wordlens:    0 7 
##              9 1 
## Senlens:     0 
##              10 
## Syllens:     0 3 
##              9 1 
\end{lstlisting}


Now, this ``model'' is based only on very simple counts, and is easily fooled.
For example, the sentence \code{S.T.A.R. Labs is a research facility in the DC
comics universe.}  would be treated as though it were 5 separate sentences.
However, the counts are constructed \emph{extremely} quickly, and so they are
still useful as a first pass in an analysis.
